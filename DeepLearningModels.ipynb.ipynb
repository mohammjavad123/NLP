{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doTGHqhrcgPR",
        "outputId": "cfbea7b3-38bf-4641-b67a-4dbf86c07ce4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-04 10:23:52--  https://www.dropbox.com/scl/fi/w80rzqopglpujsx2mryw2/semantic_tech_real50.csv?rlkey=ff2c7nf6n1rao0w2f2vzmnozm&st=amqrp3wk&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6016:18::a27d:112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucfee49b0301acb2c21d1284bf36.dl.dropboxusercontent.com/cd/0/inline/CnJ_9V9h1Fnz59pGrWyzC0gqvOAJneEydQaosLMSP4ePchMfXSSUiqKLj5V0qFmPIN4sD8CQ8swIWEhsGWobaj5RsWaX8PftpuPQ6jjUdWnRJde30tLEaZuZabx2W3vSurDmK06i9apV8qc9VZeQ0341/file?dl=1# [following]\n",
            "--2025-04-04 10:23:52--  https://ucfee49b0301acb2c21d1284bf36.dl.dropboxusercontent.com/cd/0/inline/CnJ_9V9h1Fnz59pGrWyzC0gqvOAJneEydQaosLMSP4ePchMfXSSUiqKLj5V0qFmPIN4sD8CQ8swIWEhsGWobaj5RsWaX8PftpuPQ6jjUdWnRJde30tLEaZuZabx2W3vSurDmK06i9apV8qc9VZeQ0341/file?dl=1\n",
            "Resolving ucfee49b0301acb2c21d1284bf36.dl.dropboxusercontent.com (ucfee49b0301acb2c21d1284bf36.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n",
            "Connecting to ucfee49b0301acb2c21d1284bf36.dl.dropboxusercontent.com (ucfee49b0301acb2c21d1284bf36.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 47923997 (46M) [application/binary]\n",
            "Saving to: ‘semantic_tech_real50.csv?rlkey=ff2c7nf6n1rao0w2f2vzmnozm&st=amqrp3wk&dl=1’\n",
            "\n",
            "semantic_tech_real5 100%[===================>]  45.70M   131MB/s    in 0.3s    \n",
            "\n",
            "2025-04-04 10:23:53 (131 MB/s) - ‘semantic_tech_real50.csv?rlkey=ff2c7nf6n1rao0w2f2vzmnozm&st=amqrp3wk&dl=1’ saved [47923997/47923997]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget 'https://www.dropbox.com/scl/fi/w80rzqopglpujsx2mryw2/semantic_tech_real50.csv?rlkey=ff2c7nf6n1rao0w2f2vzmnozm&st=amqrp3wk&dl=1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iH-avl4yZvd0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTGdFJkcZveo"
      },
      "outputs": [],
      "source": [
        "cleaned_rows = []\n",
        "with open(\"/content/semantic_tech_real50.csv\", \"r\", encoding=\"utf-8\") as f:\n",
        "    current_row = ''\n",
        "    for line in f:\n",
        "        current_row += line\n",
        "        if current_row.count('\"') % 2 == 0:  # line is complete if quotes are balanced\n",
        "            cleaned_rows.append(current_row.strip())\n",
        "            current_row = ''\n",
        "\n",
        "# Save cleaned content to a temp file\n",
        "with open(\"cleaned_sentiment.csv\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(cleaned_rows))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnXcWfXkZx5x"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHnlPx1eZx6t",
        "outputId": "7cd63424-6a5f-4606-ed52-6c76a0032620"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                  review_title  \\\n",
            "0                    Reliable and travels well   \n",
            "1  Samsung 4K T.V. Up-scales 1080 so very nice   \n",
            "2                             Perfect solution   \n",
            "3        Good 1TB Drive. SmartWare... not good   \n",
            "4                      Do Not Waste Your Money   \n",
            "\n",
            "                                         review_text    brand  sentiment  \\\n",
            "0  I've had one of the eGo 500GB Iomega drives no...    apple          1   \n",
            "1  Bought my Samsung 65 inch this weekend and I a...  samsung          1   \n",
            "2  This was the perfect solution to my predicamen...  samsung          1   \n",
            "3  I would not exaggerate if I called myself a WD...   lenovo          1   \n",
            "4  Wow!  I bought two of these for use in my depa...  samsung         -1   \n",
            "\n",
            "   star_rating  \n",
            "0          4.0  \n",
            "1          5.0  \n",
            "2          5.0  \n",
            "3          4.0  \n",
            "4          1.0  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"cleaned_sentiment.csv\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbsg43ceaT-J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRhmCnboZexQ",
        "outputId": "21492f26-52f3-4bb4-a164-71705ab8d62e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m481.3/491.2 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/183.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/143.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q  datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KI_dWQs51Qzl",
        "outputId": "0aac85a1-d40f-4771-dbb2-af66b3599b93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label distribution:\n",
            " label\n",
            "2    22722\n",
            "0    15011\n",
            "1    13035\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m)      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 83ms/step - accuracy: 0.5227 - loss: 0.9750 - val_accuracy: 0.6316 - val_loss: 0.8328\n",
            "Epoch 2/30\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 87ms/step - accuracy: 0.6361 - loss: 0.8137 - val_accuracy: 0.6629 - val_loss: 0.7660\n",
            "Epoch 3/30\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 87ms/step - accuracy: 0.7051 - loss: 0.6873 - val_accuracy: 0.6836 - val_loss: 0.7281\n",
            "Epoch 4/30\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 86ms/step - accuracy: 0.7597 - loss: 0.5933 - val_accuracy: 0.6906 - val_loss: 0.7187\n",
            "Epoch 5/30\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 88ms/step - accuracy: 0.7919 - loss: 0.5220 - val_accuracy: 0.6867 - val_loss: 0.7614\n",
            "Epoch 6/30\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 89ms/step - accuracy: 0.8351 - loss: 0.4348 - val_accuracy: 0.6719 - val_loss: 0.7986\n",
            "Epoch 7/30\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 87ms/step - accuracy: 0.8741 - loss: 0.3495 - val_accuracy: 0.6817 - val_loss: 0.9059\n",
            "Epoch 8/30\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 88ms/step - accuracy: 0.9012 - loss: 0.2851 - val_accuracy: 0.6742 - val_loss: 1.0027\n",
            "Epoch 9/30\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 89ms/step - accuracy: 0.9261 - loss: 0.2150 - val_accuracy: 0.6779 - val_loss: 1.0848\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.6918 - loss: 0.7070\n",
            "✅ Test Loss: 0.7187, Test Accuracy: 0.6906\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374ms/step\n",
            "💬 The new iPhone is amazing and super fast.\n",
            "➡️  Predicted Sentiment: positive\n",
            "\n",
            "💬 Samsung's battery life is terrible.\n",
            "➡️  Predicted Sentiment: negative\n",
            "\n",
            "💬 It’s okay, nothing special honestly.\n",
            "➡️  Predicted Sentiment: neutral\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# ======================\n",
        "# 🔹 STEP 1: Load and clean the dataset\n",
        "# ======================\n",
        "def load_clean_csv(file_path):\n",
        "    cleaned_rows = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        header = next(reader)\n",
        "        for row in reader:\n",
        "            if len(row) == len(header):\n",
        "                cleaned_rows.append(row)\n",
        "    df = pd.DataFrame(cleaned_rows, columns=header)\n",
        "    return df\n",
        "\n",
        "file_path = \"/content/semantic_tech_real50.csv\"\n",
        "data = load_clean_csv(file_path)\n",
        "\n",
        "# ======================\n",
        "# 🔹 STEP 2: Extract relevant columns\n",
        "# ======================\n",
        "data = data[['review_text', 'sentiment']]\n",
        "data.columns = ['text', 'label']\n",
        "\n",
        "# Drop rows with missing values\n",
        "data = data.dropna(subset=['text', 'label'])\n",
        "data['text'] = data['text'].astype(str).str.strip()\n",
        "\n",
        "# ======================\n",
        "# 🔹 STEP 3: Convert labels\n",
        "# ======================\n",
        "# From -1, 0, 1 → shift to 0, 1, 2 for one-hot encoding\n",
        "data['label'] = data['label'].astype(int) + 1\n",
        "data = data[data['label'].isin([0, 1, 2])]  # ensure valid\n",
        "\n",
        "# Optional: Check class distribution\n",
        "print(\"Label distribution:\\n\", data['label'].value_counts())\n",
        "\n",
        "# ======================\n",
        "# 🔹 STEP 4: Preprocess text\n",
        "# ======================\n",
        "comments = data['text'].values\n",
        "labels = to_categorical(data['label'].values, num_classes=3)\n",
        "\n",
        "VOCAB_SIZE = 30000\n",
        "EMBEDDING_DIM = 300\n",
        "MAX_LENGTH = 200\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(comments, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
        "X_test_padded = pad_sequences(X_test_seq, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
        "\n",
        "# ======================\n",
        "# 🔹 STEP 5: Define LSTM model\n",
        "# ======================\n",
        "model = Sequential([\n",
        "    Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH),\n",
        "    Bidirectional(LSTM(256, return_sequences=True, activation='tanh')),\n",
        "    Dropout(0.5),\n",
        "    Bidirectional(LSTM(128, activation='tanh')),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# ======================\n",
        "# 🔹 STEP 6: Train model\n",
        "# ======================\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_padded, y_train,\n",
        "    validation_data=(X_test_padded, y_test),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ======================\n",
        "# 🔹 STEP 7: Evaluate model\n",
        "# ======================\n",
        "test_loss, test_accuracy = model.evaluate(X_test_padded, y_test, verbose=1)\n",
        "print(f\"✅ Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "model.save(\"lstm_sentiment_model_apple_samsung.keras\")\n",
        "\n",
        "# ======================\n",
        "# 🔹 STEP 8: Predict on sample reviews\n",
        "# ======================\n",
        "sample_comments = [\n",
        "    \"The new iPhone is amazing and super fast.\",\n",
        "    \"Samsung's battery life is terrible.\",\n",
        "    \"It’s okay, nothing special honestly.\"\n",
        "]\n",
        "\n",
        "sample_seq = tokenizer.texts_to_sequences(sample_comments)\n",
        "sample_pad = pad_sequences(sample_seq, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
        "predictions = model.predict(sample_pad)\n",
        "\n",
        "# Map back from class index to label: 0→negative, 1→neutral, 2→positive\n",
        "label_decode = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "predicted_labels = [label_decode[i] for i in predicted_classes]\n",
        "\n",
        "for text, pred in zip(sample_comments, predicted_labels):\n",
        "    print(f\"💬 {text}\\n➡️  Predicted Sentiment: {pred}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWyc-8TR35gE"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BMxV785D35sT",
        "outputId": "93a735e0-6505-4bf7-a0ef-ccb4f9e39233"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m)      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_3 (\u001b[38;5;33mBidirectional\u001b[0m)      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 90ms/step - accuracy: 0.5289 - loss: 0.9759 - val_accuracy: 0.6547 - val_loss: 0.7812\n",
            "Epoch 2/30\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 87ms/step - accuracy: 0.6641 - loss: 0.7616 - val_accuracy: 0.6768 - val_loss: 0.7375\n",
            "Epoch 3/30\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 87ms/step - accuracy: 0.7194 - loss: 0.6507 - val_accuracy: 0.6526 - val_loss: 0.7778\n",
            "Epoch 4/30\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 87ms/step - accuracy: 0.7771 - loss: 0.5449 - val_accuracy: 0.6960 - val_loss: 0.7017\n",
            "Epoch 5/30\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 89ms/step - accuracy: 0.8268 - loss: 0.4490 - val_accuracy: 0.6903 - val_loss: 0.8115\n",
            "Epoch 6/30\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 88ms/step - accuracy: 0.8662 - loss: 0.3623 - val_accuracy: 0.6847 - val_loss: 0.9006\n",
            "Epoch 7/30\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 87ms/step - accuracy: 0.9000 - loss: 0.2783 - val_accuracy: 0.6779 - val_loss: 1.0477\n",
            "Epoch 8/30\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 87ms/step - accuracy: 0.9263 - loss: 0.2196 - val_accuracy: 0.6567 - val_loss: 1.1776\n",
            "Epoch 9/30\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 87ms/step - accuracy: 0.9450 - loss: 0.1687 - val_accuracy: 0.6755 - val_loss: 1.2351\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.6957 - loss: 0.6900\n",
            "✅ Test Loss: 0.7017, Test Accuracy: 0.6960\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step\n",
            "🔍 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.65      0.75      0.70      2997\n",
            "     Neutral       0.53      0.43      0.47      2611\n",
            "    Positive       0.80      0.82      0.81      4546\n",
            "\n",
            "    accuracy                           0.70     10154\n",
            "   macro avg       0.66      0.66      0.66     10154\n",
            "weighted avg       0.69      0.70      0.69     10154\n",
            "\n",
            "🔍 Confusion Matrix:\n",
            "[[2246  483  268]\n",
            " [ 869 1111  631]\n",
            " [ 337  499 3710]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "💬 The Galaxy S24 is fast but heats up.\n",
            "➡️  Probabilities: [0.08534862 0.49317715 0.4214742 ]\n",
            "➡️  Predicted Label: 0\n",
            "\n",
            "💬 iPhone 15 Pro Max is incredible!\n",
            "➡️  Probabilities: [0.05532453 0.08167563 0.86299986]\n",
            "➡️  Predicted Label: 1\n",
            "\n",
            "💬 Not good, not bad, just average.\n",
            "➡️  Probabilities: [0.4101152  0.52455896 0.06532583]\n",
            "➡️  Predicted Label: 0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import csv\n",
        "\n",
        "# ============================================================\n",
        "# 🔹 WHAT THIS MODEL DOES:\n",
        "# This is a BiLSTM-based sentiment classifier (like the first one),\n",
        "# but with added metrics: confusion matrix + classification report.\n",
        "# ============================================================\n",
        "\n",
        "# ======================\n",
        "# ✅ STEP 1: Load and clean your dataset\n",
        "# ======================\n",
        "def load_clean_csv(file_path):\n",
        "    cleaned_rows = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        header = next(reader)\n",
        "        for row in reader:\n",
        "            if len(row) == len(header):\n",
        "                cleaned_rows.append(row)\n",
        "    df = pd.DataFrame(cleaned_rows, columns=header)\n",
        "    return df\n",
        "\n",
        "file_path = \"/content/semantic_tech_real50.csv\"\n",
        "data = load_clean_csv(file_path)\n",
        "\n",
        "# Use only the relevant columns\n",
        "data = data[['review_text', 'sentiment']]\n",
        "data.columns = ['text', 'label']\n",
        "\n",
        "# Clean text and label columns\n",
        "data = data.dropna(subset=['text', 'label'])\n",
        "data['text'] = data['text'].astype(str).str.strip()\n",
        "data['label'] = data['label'].astype(int) + 1  # Convert from -1,0,1 → 0,1,2\n",
        "data = data[data['label'].isin([0, 1, 2])]\n",
        "\n",
        "# ======================\n",
        "# ✅ STEP 2: Prepare text and labels\n",
        "# ======================\n",
        "comments = data['text'].values\n",
        "labels = to_categorical(data['label'].values, num_classes=3)\n",
        "\n",
        "# Parameters\n",
        "VOCAB_SIZE = 30000\n",
        "EMBEDDING_DIM = 300\n",
        "MAX_LENGTH = 200\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(comments, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenization and padding\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
        "X_test_padded = pad_sequences(X_test_seq, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
        "\n",
        "# ======================\n",
        "# ✅ STEP 3: Build the BiLSTM Model\n",
        "# (Same architecture as previous model)\n",
        "# ======================\n",
        "model = Sequential([\n",
        "    Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH),\n",
        "    Bidirectional(LSTM(256, activation='tanh', return_sequences=True)),\n",
        "    Dropout(0.5),\n",
        "    Bidirectional(LSTM(128, activation='tanh')),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# ======================\n",
        "# ✅ STEP 4: Train the model\n",
        "# ======================\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_padded, y_train,\n",
        "    validation_data=(X_test_padded, y_test),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ======================\n",
        "# ✅ STEP 5: Evaluate with accuracy + metrics\n",
        "# ======================\n",
        "test_loss, test_accuracy = model.evaluate(X_test_padded, y_test, verbose=1)\n",
        "print(f\"✅ Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Save model\n",
        "model.save(\"lstm_sentiment_model_with_metrics.keras\")\n",
        "\n",
        "# ======================\n",
        "# ✅ STEP 6: Classification Report + Confusion Matrix\n",
        "# ======================\n",
        "y_pred_probs = model.predict(X_test_padded)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"🔍 Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=['Negative', 'Neutral', 'Positive']))\n",
        "\n",
        "print(\"🔍 Confusion Matrix:\")\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "# ======================\n",
        "# ✅ STEP 7: Run on sample reviews\n",
        "# ======================\n",
        "sample_comments = [\n",
        "    \"The Galaxy S24 is fast but heats up.\",\n",
        "    \"iPhone 15 Pro Max is incredible!\",\n",
        "    \"Not good, not bad, just average.\"\n",
        "]\n",
        "\n",
        "sample_seq = tokenizer.texts_to_sequences(sample_comments)\n",
        "sample_pad = pad_sequences(sample_seq, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
        "\n",
        "predictions = model.predict(sample_pad)\n",
        "predicted_labels = np.argmax(predictions, axis=1) - 1  # back to -1, 0, 1\n",
        "\n",
        "for text, prob, label in zip(sample_comments, predictions, predicted_labels):\n",
        "    print(f\"💬 {text}\")\n",
        "    print(f\"➡️  Probabilities: {prob}\")\n",
        "    print(f\"➡️  Predicted Label: {label}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP8-qHM05lnU"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbxuydrV5lwz",
        "outputId": "dfe60d22-4974-4238-f31d-b8d558915c17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔁 Fold 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Validation Loss: 0.9633, Accuracy: 0.6833\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step\n",
            "📊 F1-Score: 0.6824\n",
            "\n",
            "🔁 Fold 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Validation Loss: 0.9363, Accuracy: 0.6760\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step\n",
            "📊 F1-Score: 0.6798\n",
            "\n",
            "🔁 Fold 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Validation Loss: 0.9287, Accuracy: 0.6894\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step\n",
            "📊 F1-Score: 0.6910\n",
            "\n",
            "🔁 Fold 4/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Validation Loss: 0.9950, Accuracy: 0.6731\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step\n",
            "📊 F1-Score: 0.6707\n",
            "\n",
            "🔁 Fold 5/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Validation Loss: 0.8940, Accuracy: 0.6867\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step\n",
            "📊 F1-Score: 0.6860\n",
            "\n",
            "📈 Cross-Validation Summary:\n",
            "Mean Accuracy: 0.6817 ± 0.0062\n",
            "Mean F1-Score: 0.6820 ± 0.0068\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, Conv1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# =============================================================\n",
        "# 🔍 WHAT THIS DOES:\n",
        "# - Adds Conv1D + GlobalMaxPooling1D before BiLSTM\n",
        "# - Uses 5-fold cross-validation\n",
        "# - Collects accuracy, F1-score, and classification report\n",
        "# - Trains a new model from scratch for each fold\n",
        "# =============================================================\n",
        "\n",
        "# ======================\n",
        "# ✅ Load and clean the dataset\n",
        "# ======================\n",
        "def load_clean_csv(file_path):\n",
        "    cleaned_rows = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        header = next(reader)\n",
        "        for row in reader:\n",
        "            if len(row) == len(header):\n",
        "                cleaned_rows.append(row)\n",
        "    df = pd.DataFrame(cleaned_rows, columns=header)\n",
        "    return df\n",
        "\n",
        "file_path = \"/content/semantic_tech_real50.csv\"\n",
        "data = load_clean_csv(file_path)\n",
        "\n",
        "# Use relevant columns\n",
        "data = data[['review_text', 'sentiment']]\n",
        "data.columns = ['text', 'label']\n",
        "data = data.dropna(subset=['text', 'label'])\n",
        "data['text'] = data['text'].astype(str).str.strip()\n",
        "data['label'] = data['label'].astype(int) + 1  # Shift labels: -1,0,1 → 0,1,2\n",
        "data = data[data['label'].isin([0, 1, 2])]\n",
        "\n",
        "# ======================\n",
        "# ✅ Set parameters and tokenize text\n",
        "# ======================\n",
        "VOCAB_SIZE = 30000\n",
        "EMBEDDING_DIM = 300\n",
        "MAX_LENGTH = 200\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "texts = data['text'].values\n",
        "labels = data['label'].values\n",
        "\n",
        "# Tokenizer should be fit once on all data\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# ======================\n",
        "# ✅ K-Fold Cross-Validation setup\n",
        "# ======================\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "fold_accuracies = []\n",
        "fold_f1_scores = []\n",
        "fold_reports = []\n",
        "\n",
        "# ======================\n",
        "# 🔄 Perform K-Fold loop\n",
        "# ======================\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(data)):\n",
        "    print(f\"\\n🔁 Fold {fold + 1}/{kf.get_n_splits()}\")\n",
        "\n",
        "    # Split data for this fold\n",
        "    train_data = data.iloc[train_index]\n",
        "    val_data = data.iloc[val_index]\n",
        "\n",
        "    X_train_fold = train_data['text']\n",
        "    y_train_fold = train_data['label']\n",
        "    X_val_fold = val_data['text']\n",
        "    y_val_fold = val_data['label']\n",
        "\n",
        "    # Tokenize and pad\n",
        "    X_train_seq = tokenizer.texts_to_sequences(X_train_fold)\n",
        "    X_val_seq = tokenizer.texts_to_sequences(X_val_fold)\n",
        "    X_train_padded = pad_sequences(X_train_seq, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
        "    X_val_padded = pad_sequences(X_val_seq, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
        "\n",
        "    # Convert labels to one-hot\n",
        "    y_train_categorical = to_categorical(y_train_fold, num_classes=3)\n",
        "    y_val_categorical = to_categorical(y_val_fold, num_classes=3)\n",
        "\n",
        "    # ======================\n",
        "    # ✅ Define the model (Conv1D + BiLSTM + GlobalMaxPooling)\n",
        "    # ======================\n",
        "    model = Sequential([\n",
        "        Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH),\n",
        "        Conv1D(128, 5, activation='relu'),                          # ✅ New layer: extract local patterns\n",
        "        Bidirectional(LSTM(128, return_sequences=True)),            # Bidirectional LSTM layer\n",
        "        GlobalMaxPooling1D(),                                       # ✅ New layer: reduces sequence to vector\n",
        "        Dropout(0.5),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(3, activation='softmax')                              # Softmax for 3-class classification\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(\n",
        "        X_train_padded, y_train_categorical,\n",
        "        epochs=5,                        # 🔁 Fewer epochs to keep training light per fold\n",
        "        batch_size=BATCH_SIZE,\n",
        "        verbose=0                        # 🤫 Silent training to avoid clutter\n",
        "    )\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    val_loss, val_accuracy = model.evaluate(X_val_padded, y_val_categorical, verbose=0)\n",
        "    print(f\"✅ Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Predict and compute metrics\n",
        "    y_val_pred_probs = model.predict(X_val_padded)\n",
        "    y_val_pred = np.argmax(y_val_pred_probs, axis=1)\n",
        "    y_val_true = y_val_fold.values.astype(int)\n",
        "\n",
        "    fold_f1 = f1_score(y_val_true, y_val_pred, average='weighted')\n",
        "    print(f\"📊 F1-Score: {fold_f1:.4f}\")\n",
        "\n",
        "    # Store metrics\n",
        "    fold_accuracies.append(val_accuracy)\n",
        "    fold_f1_scores.append(fold_f1)\n",
        "    fold_reports.append(classification_report(\n",
        "        y_val_true, y_val_pred,\n",
        "        target_names=['Negative', 'Neutral', 'Positive'],\n",
        "        output_dict=True\n",
        "    ))\n",
        "\n",
        "# ======================\n",
        "# ✅ Final summary across all folds\n",
        "# ======================\n",
        "print(\"\\n📈 Cross-Validation Summary:\")\n",
        "print(f\"Mean Accuracy: {np.mean(fold_accuracies):.4f} ± {np.std(fold_accuracies):.4f}\")\n",
        "print(f\"Mean F1-Score: {np.mean(fold_f1_scores):.4f} ± {np.std(fold_f1_scores):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re0-51mG7F6k"
      },
      "source": [
        "Here's your GRU + Stratified K-Fold code again, now with inline comments that explain exactly how it works, and how it's different from the previous LSTM and Conv1D+LSTM models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6wdL3x37GD6",
        "outputId": "0060297f-8fc9-4db5-a2a7-0ba51f4cb4af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class Weights: {0: np.float64(1.127351053671752), 1: np.float64(1.2982483058432426), 2: np.float64(0.7447701200011736)}\n",
            "\n",
            "📁 Fold 1/5\n",
            "Epoch 1/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 31ms/step - accuracy: 0.3442 - loss: 1.1006\n",
            "Epoch 2/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 31ms/step - accuracy: 0.4141 - loss: 1.0902\n",
            "Epoch 3/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 30ms/step - accuracy: 0.4645 - loss: 1.0751\n",
            "Epoch 4/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.4890 - loss: 1.0500\n",
            "Epoch 5/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.5403 - loss: 0.9878\n",
            "Epoch 6/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - accuracy: 0.7096 - loss: 0.7400\n",
            "Epoch 7/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 31ms/step - accuracy: 0.7875 - loss: 0.5856\n",
            "Epoch 8/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - accuracy: 0.8442 - loss: 0.4638\n",
            "Epoch 9/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - accuracy: 0.8845 - loss: 0.3669\n",
            "Epoch 10/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.9160 - loss: 0.2895\n",
            "Epoch 11/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.9371 - loss: 0.2288\n",
            "Epoch 12/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 30ms/step - accuracy: 0.9504 - loss: 0.1842\n",
            "Epoch 13/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.9613 - loss: 0.1485\n",
            "Epoch 14/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 36ms/step - accuracy: 0.9686 - loss: 0.1211\n",
            "Epoch 15/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 37ms/step - accuracy: 0.9740 - loss: 0.1008\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.6380 - loss: 2.2898\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step\n",
            "📊 Fold 1: Accuracy = 0.6348, F1 = 0.6425\n",
            "\n",
            "📁 Fold 2/5\n",
            "Epoch 1/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.3313 - loss: 1.1012\n",
            "Epoch 2/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 34ms/step - accuracy: 0.3872 - loss: 1.0961\n",
            "Epoch 3/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 30ms/step - accuracy: 0.4484 - loss: 1.0833\n",
            "Epoch 4/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.4791 - loss: 1.0614\n",
            "Epoch 5/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.5555 - loss: 0.9838\n",
            "Epoch 6/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - accuracy: 0.7196 - loss: 0.7252\n",
            "Epoch 7/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.8017 - loss: 0.5602\n",
            "Epoch 8/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.8596 - loss: 0.4396\n",
            "Epoch 9/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 30ms/step - accuracy: 0.8997 - loss: 0.3254\n",
            "Epoch 10/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 39ms/step - accuracy: 0.9313 - loss: 0.2459\n",
            "Epoch 11/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 36ms/step - accuracy: 0.9476 - loss: 0.1936\n",
            "Epoch 12/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 31ms/step - accuracy: 0.9599 - loss: 0.1505\n",
            "Epoch 13/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 30ms/step - accuracy: 0.9706 - loss: 0.1175\n",
            "Epoch 14/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.9746 - loss: 0.1024\n",
            "Epoch 15/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - accuracy: 0.9799 - loss: 0.0820\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6294 - loss: 2.3360\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step\n",
            "📊 Fold 2: Accuracy = 0.6307, F1 = 0.6375\n",
            "\n",
            "📁 Fold 3/5\n",
            "Epoch 1/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 31ms/step - accuracy: 0.3395 - loss: 1.1041\n",
            "Epoch 2/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - accuracy: 0.3556 - loss: 1.0967\n",
            "Epoch 3/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - accuracy: 0.4228 - loss: 1.0847\n",
            "Epoch 4/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.4792 - loss: 1.0590\n",
            "Epoch 5/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.5125 - loss: 1.0161\n",
            "Epoch 6/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - accuracy: 0.5349 - loss: 0.9607\n",
            "Epoch 7/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.6595 - loss: 0.8271\n",
            "Epoch 8/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 31ms/step - accuracy: 0.7643 - loss: 0.6224\n",
            "Epoch 9/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - accuracy: 0.8268 - loss: 0.4820\n",
            "Epoch 10/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.8689 - loss: 0.3899\n",
            "Epoch 11/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 31ms/step - accuracy: 0.9064 - loss: 0.2943\n",
            "Epoch 12/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - accuracy: 0.9334 - loss: 0.2216\n",
            "Epoch 13/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.9492 - loss: 0.1757\n",
            "Epoch 14/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 31ms/step - accuracy: 0.9613 - loss: 0.1343\n",
            "Epoch 15/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 30ms/step - accuracy: 0.9711 - loss: 0.1099\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.6255 - loss: 2.0407\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step\n",
            "📊 Fold 3: Accuracy = 0.6272, F1 = 0.6341\n",
            "\n",
            "📁 Fold 4/5\n",
            "Epoch 1/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 30ms/step - accuracy: 0.3551 - loss: 1.0998\n",
            "Epoch 2/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.3960 - loss: 1.0939\n",
            "Epoch 3/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 31ms/step - accuracy: 0.4560 - loss: 1.0719\n",
            "Epoch 4/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - accuracy: 0.4992 - loss: 1.0347\n",
            "Epoch 5/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.5282 - loss: 0.9872\n",
            "Epoch 6/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 30ms/step - accuracy: 0.6693 - loss: 0.8216\n",
            "Epoch 7/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - accuracy: 0.7720 - loss: 0.6143\n",
            "Epoch 8/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.8413 - loss: 0.4695\n",
            "Epoch 9/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 31ms/step - accuracy: 0.8866 - loss: 0.3487\n",
            "Epoch 10/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - accuracy: 0.9178 - loss: 0.2700\n",
            "Epoch 11/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 31ms/step - accuracy: 0.9424 - loss: 0.2017\n",
            "Epoch 12/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 31ms/step - accuracy: 0.9553 - loss: 0.1647\n",
            "Epoch 13/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - accuracy: 0.9659 - loss: 0.1249\n",
            "Epoch 14/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 31ms/step - accuracy: 0.9719 - loss: 0.1043\n",
            "Epoch 15/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 31ms/step - accuracy: 0.9778 - loss: 0.0931\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6362 - loss: 2.5049\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step\n",
            "📊 Fold 4: Accuracy = 0.6405, F1 = 0.6440\n",
            "\n",
            "📁 Fold 5/5\n",
            "Epoch 1/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 30ms/step - accuracy: 0.3476 - loss: 1.1010\n",
            "Epoch 2/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 31ms/step - accuracy: 0.4096 - loss: 1.0895\n",
            "Epoch 3/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - accuracy: 0.4425 - loss: 1.0788\n",
            "Epoch 4/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - accuracy: 0.4988 - loss: 1.0471\n",
            "Epoch 5/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.6755 - loss: 0.8146\n",
            "Epoch 6/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 31ms/step - accuracy: 0.7622 - loss: 0.6364\n",
            "Epoch 7/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - accuracy: 0.8291 - loss: 0.4995\n",
            "Epoch 8/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.8824 - loss: 0.3875\n",
            "Epoch 9/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 31ms/step - accuracy: 0.9184 - loss: 0.2881\n",
            "Epoch 10/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - accuracy: 0.9357 - loss: 0.2352\n",
            "Epoch 11/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.9533 - loss: 0.1787\n",
            "Epoch 12/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.9639 - loss: 0.1489\n",
            "Epoch 13/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - accuracy: 0.9744 - loss: 0.1137\n",
            "Epoch 14/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.9755 - loss: 0.1028\n",
            "Epoch 15/15\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 31ms/step - accuracy: 0.9810 - loss: 0.0879\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.6371 - loss: 2.4171\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step\n",
            "📊 Fold 5: Accuracy = 0.6408, F1 = 0.6460\n",
            "\n",
            "✅ Stratified K-Fold Summary:\n",
            "Mean Accuracy: 0.6348 ± 0.0053\n",
            "Mean F1-Score: 0.6408 ± 0.0044\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# ======================\n",
        "# ✅ Load and clean the dataset\n",
        "# ======================\n",
        "def load_clean_csv(file_path):\n",
        "    cleaned_rows = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        header = next(reader)\n",
        "        for row in reader:\n",
        "            if len(row) == len(header):\n",
        "                cleaned_rows.append(row)\n",
        "    df = pd.DataFrame(cleaned_rows, columns=header)\n",
        "    return df\n",
        "\n",
        "file_path = \"/content/semantic_tech_real50.csv\"\n",
        "data = load_clean_csv(file_path)\n",
        "\n",
        "# Take relevant columns\n",
        "data = data[['review_text', 'sentiment']]\n",
        "data.columns = ['text', 'label']\n",
        "data = data.dropna(subset=['text', 'label'])\n",
        "data['text'] = data['text'].astype(str).str.strip()\n",
        "data['label'] = data['label'].astype(int) + 1  # from (-1,0,1) → (0,1,2)\n",
        "data = data[data['label'].isin([0, 1, 2])]\n",
        "\n",
        "# ======================\n",
        "# ✅ Parameters :))\n",
        "# ======================\n",
        "VOCAB_SIZE = 30000\n",
        "EMBEDDING_DIM = 300\n",
        "MAX_LENGTH = 200\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 15  # More training for GRU :))\n",
        "LEARNING_RATE = 0.0001  # Smaller learning rate = more stable :))\n",
        "\n",
        "# ======================\n",
        "# ✅ Prepare data and tokenize\n",
        "# ======================\n",
        "comments = data['text'].values\n",
        "labels = to_categorical(data['label'].values, num_classes=3)  # One-hot :))\n",
        "y_classes = np.argmax(labels, axis=1)  # Back to class labels for stratified split :))\n",
        "\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(comments)  # Only fit once :))\n",
        "\n",
        "# ======================\n",
        "# ✅ Class weights (for imbalance) :))\n",
        "# ======================\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_classes), y=y_classes)\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "print(\"Class Weights:\", class_weights)\n",
        "\n",
        "# ======================\n",
        "# ✅ Stratified K-Fold setup :))\n",
        "# ======================\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "fold_accuracies, fold_f1_scores = [], []\n",
        "\n",
        "# ======================\n",
        "# 🔁 Start fold loop :))\n",
        "# ======================\n",
        "for fold, (train_index, val_index) in enumerate(skf.split(comments, y_classes)):\n",
        "    print(f\"\\n📁 Fold {fold + 1}/5\")\n",
        "\n",
        "    # Split comments and labels\n",
        "    X_train_fold, X_val_fold = comments[train_index], comments[val_index]\n",
        "    y_train_fold, y_val_fold = labels[train_index], labels[val_index]\n",
        "\n",
        "    # Tokenize and pad sequences\n",
        "    X_train_seq = tokenizer.texts_to_sequences(X_train_fold)\n",
        "    X_val_seq = tokenizer.texts_to_sequences(X_val_fold)\n",
        "    X_train_padded = pad_sequences(X_train_seq, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
        "    X_val_padded = pad_sequences(X_val_seq, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
        "\n",
        "    # ======================\n",
        "    # ✅ Build the GRU model for this fold :))\n",
        "    # ======================\n",
        "    model = Sequential([\n",
        "        Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH),\n",
        "        Bidirectional(GRU(64, activation='tanh', return_sequences=True)),  # Smaller GRU for stability :))\n",
        "        Dropout(0.6),  # Strong regularization :))\n",
        "        GRU(32, activation='tanh'),\n",
        "        Dropout(0.6),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.6),\n",
        "        Dense(3, activation='softmax')  # Softmax for multi-class :))\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',  # You can swap in Adam(lr=LEARNING_RATE) if needed\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # ======================\n",
        "    # ✅ Train the model :))\n",
        "    # ======================\n",
        "    model.fit(\n",
        "        X_train_padded, y_train_fold,\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_weight=class_weights,  # handle imbalanced classes :))\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # ======================\n",
        "    # ✅ Evaluate and collect results :))\n",
        "    # ======================\n",
        "    val_loss, val_accuracy = model.evaluate(X_val_padded, y_val_fold, verbose=1)\n",
        "    y_val_pred_probs = model.predict(X_val_padded)\n",
        "    y_val_pred = np.argmax(y_val_pred_probs, axis=1)\n",
        "    y_val_true = np.argmax(y_val_fold, axis=1)\n",
        "\n",
        "    f1 = f1_score(y_val_true, y_val_pred, average='weighted')\n",
        "    print(f\"📊 Fold {fold + 1}: Accuracy = {val_accuracy:.4f}, F1 = {f1:.4f}\")\n",
        "\n",
        "    fold_accuracies.append(val_accuracy)\n",
        "    fold_f1_scores.append(f1)\n",
        "\n",
        "# ======================\n",
        "# ✅ Final cross-validation results summary :))\n",
        "# ======================\n",
        "print(\"\\n✅ Stratified K-Fold Summary:\")\n",
        "print(f\"Mean Accuracy: {np.mean(fold_accuracies):.4f} ± {np.std(fold_accuracies):.4f}\")\n",
        "print(f\"Mean F1-Score: {np.mean(fold_f1_scores):.4f} ± {np.std(fold_f1_scores):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZNCM3_9-0sQ"
      },
      "source": [
        "herererererererererererererrererer*✅* GRU Model with Pre-trained GloVe Embeddings + Stratified K-Fold\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qpd1KM3L_itP",
        "outputId": "8e9e0c1c-26f4-428e-cf33-e072bed2a47e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "Found existing installation: scipy 1.14.1\n",
            "Uninstalling scipy-1.14.1:\n",
            "  Successfully uninstalled scipy-1.14.1\n",
            "\u001b[33mWARNING: Skipping gensim as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting numpy\n",
            "  Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy\n",
            "  Downloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, tensorboard, scipy, ml-dtypes, tensorflow\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.3 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ml-dtypes-0.5.1 numpy-2.1.3 scipy-1.15.2 tensorboard-2.19.0 tensorflow-2.19.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y numpy scipy gensim\n",
        "!pip install --upgrade numpy scipy tensorflow scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik2ImZ34D02g"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)  # Force kernel restart\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qFcscgbDjO7",
        "outputId": "c2990541-6c40-4f3e-de1f-c98727d172ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y numpy\n",
        "!pip install numpy==1.26.4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uu_WiOYd-04u",
        "outputId": "1a8abe4b-c9fc-4b7b-bb8d-b4df1ab66940"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📁 Fold 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 25ms/step - accuracy: 0.4420 - loss: 1.0807\n",
            "Epoch 2/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 26ms/step - accuracy: 0.4672 - loss: 1.0564\n",
            "Epoch 3/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.4458 - loss: 1.0690\n",
            "Epoch 4/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 28ms/step - accuracy: 0.5849 - loss: 0.8795\n",
            "Epoch 5/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 25ms/step - accuracy: 0.6576 - loss: 0.7564\n",
            "Epoch 6/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.6961 - loss: 0.6973\n",
            "Epoch 7/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.7192 - loss: 0.6549\n",
            "Epoch 8/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 28ms/step - accuracy: 0.7337 - loss: 0.6167\n",
            "Epoch 9/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 25ms/step - accuracy: 0.7564 - loss: 0.5773\n",
            "Epoch 10/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 29ms/step - accuracy: 0.7653 - loss: 0.5538\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.7125 - loss: 0.6671\n",
            "✅ Fold 1 - Loss: 0.6726 | Accuracy: 0.7115\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step\n",
            "📊 Fold 1 - F1-Score: 0.7164\n",
            "\n",
            "📁 Fold 2/5\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 26ms/step - accuracy: 0.4381 - loss: 1.0816\n",
            "Epoch 2/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.4652 - loss: 1.0571\n",
            "Epoch 3/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 25ms/step - accuracy: 0.5049 - loss: 1.0268\n",
            "Epoch 4/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.5525 - loss: 0.9758\n",
            "Epoch 5/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.5920 - loss: 0.9149\n",
            "Epoch 6/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 25ms/step - accuracy: 0.6455 - loss: 0.7855\n",
            "Epoch 7/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.6839 - loss: 0.7193\n",
            "Epoch 8/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.7126 - loss: 0.6603\n",
            "Epoch 9/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 25ms/step - accuracy: 0.7288 - loss: 0.6311\n",
            "Epoch 10/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.7478 - loss: 0.5993\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.7028 - loss: 0.6893\n",
            "✅ Fold 2 - Loss: 0.6871 | Accuracy: 0.7023\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step\n",
            "📊 Fold 2 - F1-Score: 0.7001\n",
            "\n",
            "📁 Fold 3/5\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 26ms/step - accuracy: 0.4415 - loss: 1.0803\n",
            "Epoch 2/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 25ms/step - accuracy: 0.4648 - loss: 1.0568\n",
            "Epoch 3/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 25ms/step - accuracy: 0.4608 - loss: 1.0558\n",
            "Epoch 4/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 26ms/step - accuracy: 0.5517 - loss: 0.9727\n",
            "Epoch 5/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.6383 - loss: 0.7983\n",
            "Epoch 6/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.6707 - loss: 0.7384\n",
            "Epoch 7/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.6989 - loss: 0.6811\n",
            "Epoch 8/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.7262 - loss: 0.6400\n",
            "Epoch 9/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 28ms/step - accuracy: 0.7419 - loss: 0.6043\n",
            "Epoch 10/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 25ms/step - accuracy: 0.7571 - loss: 0.5793\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.7180 - loss: 0.6515\n",
            "✅ Fold 3 - Loss: 0.6596 | Accuracy: 0.7151\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step\n",
            "📊 Fold 3 - F1-Score: 0.7178\n",
            "\n",
            "📁 Fold 4/5\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 25ms/step - accuracy: 0.4327 - loss: 1.0795\n",
            "Epoch 2/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 25ms/step - accuracy: 0.4533 - loss: 1.0617\n",
            "Epoch 3/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 25ms/step - accuracy: 0.4675 - loss: 1.0566\n",
            "Epoch 4/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 25ms/step - accuracy: 0.4633 - loss: 1.0392\n",
            "Epoch 5/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.5411 - loss: 0.9457\n",
            "Epoch 6/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.6579 - loss: 0.7579\n",
            "Epoch 7/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 29ms/step - accuracy: 0.6913 - loss: 0.6972\n",
            "Epoch 8/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 25ms/step - accuracy: 0.7145 - loss: 0.6557\n",
            "Epoch 9/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.7326 - loss: 0.6214\n",
            "Epoch 10/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.7474 - loss: 0.5925\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.7111 - loss: 0.6555\n",
            "✅ Fold 4 - Loss: 0.6657 | Accuracy: 0.7104\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step\n",
            "📊 Fold 4 - F1-Score: 0.7149\n",
            "\n",
            "📁 Fold 5/5\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 25ms/step - accuracy: 0.4385 - loss: 1.0782\n",
            "Epoch 2/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 25ms/step - accuracy: 0.4607 - loss: 1.0597\n",
            "Epoch 3/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.4891 - loss: 1.0350\n",
            "Epoch 4/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.5079 - loss: 0.9840\n",
            "Epoch 5/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 28ms/step - accuracy: 0.6305 - loss: 0.8046\n",
            "Epoch 6/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 26ms/step - accuracy: 0.6630 - loss: 0.7533\n",
            "Epoch 7/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.6824 - loss: 0.7175\n",
            "Epoch 8/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.7025 - loss: 0.6817\n",
            "Epoch 9/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 25ms/step - accuracy: 0.7169 - loss: 0.6574\n",
            "Epoch 10/10\n",
            "\u001b[1m635/635\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.7345 - loss: 0.6236\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.6974 - loss: 0.6860\n",
            "✅ Fold 5 - Loss: 0.6746 | Accuracy: 0.7027\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step\n",
            "📊 Fold 5 - F1-Score: 0.7004\n",
            "\n",
            "📈 Final GloVe GRU Model Results:\n",
            "Mean Accuracy: 0.7084 ± 0.0051\n",
            "Mean F1-Score: 0.7099 ± 0.0080\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# ======================\n",
        "# ✅ 1. Load and clean your dataset\n",
        "# ======================\n",
        "def load_clean_csv(file_path):\n",
        "    cleaned_rows = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        header = next(reader)\n",
        "        for row in reader:\n",
        "            if len(row) == len(header):\n",
        "                cleaned_rows.append(row)\n",
        "    return pd.DataFrame(cleaned_rows, columns=header)\n",
        "\n",
        "file_path = \"/content/semantic_tech_real50.csv\"\n",
        "data = load_clean_csv(file_path)\n",
        "\n",
        "# Use the relevant columns\n",
        "data = data[['review_text', 'sentiment']]\n",
        "data.columns = ['text', 'label']\n",
        "data = data.dropna()\n",
        "data['text'] = data['text'].astype(str).str.strip()\n",
        "data['label'] = data['label'].astype(int) + 1  # Shift from -1,0,1 → 0,1,2\n",
        "\n",
        "# ======================\n",
        "# ✅ 2. Tokenization\n",
        "# ======================\n",
        "VOCAB_SIZE = 30000\n",
        "EMBEDDING_DIM = 300\n",
        "MAX_LENGTH = 200\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "\n",
        "comments = data['text'].values\n",
        "labels = to_categorical(data['label'].values, num_classes=3)\n",
        "y_classes = np.argmax(labels, axis=1)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(comments)\n",
        "\n",
        "# ======================\n",
        "# ✅ 3. Load GloVe embeddings manually\n",
        "# Download from: https://nlp.stanford.edu/data/glove.6B.zip → use glove.6B.300d.txt\n",
        "# ======================\n",
        "def load_glove_embeddings(file_path, tokenizer, vocab_size=30000, embedding_dim=300):\n",
        "    embeddings_index = {}\n",
        "    with open(file_path, encoding=\"utf8\") as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if i < vocab_size:\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix\n",
        "\n",
        "embedding_matrix = load_glove_embeddings(\"glove.6B.300d.txt\", tokenizer)\n",
        "\n",
        "# ======================\n",
        "# ✅ 4. Stratified K-Fold + Model Training\n",
        "# ======================\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_accuracies, fold_f1_scores = [], []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(comments, y_classes)):\n",
        "    print(f\"\\n📁 Fold {fold + 1}/5\")\n",
        "\n",
        "    X_train, X_val = comments[train_idx], comments[val_idx]\n",
        "    y_train, y_val = labels[train_idx], labels[val_idx]\n",
        "\n",
        "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "    X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
        "\n",
        "    X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LENGTH, padding='post')\n",
        "    X_val_pad = pad_sequences(X_val_seq, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "    # ======================\n",
        "    # ✅ GRU Model using Pre-trained GloVe embeddings (trainable=False)\n",
        "    # ======================\n",
        "    model = Sequential([\n",
        "        Embedding(\n",
        "            VOCAB_SIZE, EMBEDDING_DIM,\n",
        "            embeddings_initializer=Constant(embedding_matrix),\n",
        "            input_length=MAX_LENGTH,\n",
        "            trainable=False  # ❄️ GloVe frozen (not fine-tuned)\n",
        "        ),\n",
        "        Bidirectional(GRU(64, return_sequences=True, activation='tanh')),\n",
        "        Dropout(0.5),\n",
        "        GRU(32, activation='tanh'),\n",
        "        Dropout(0.5),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(3, activation='softmax')  # 3 sentiment classes\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train\n",
        "    model.fit(X_train_pad, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1)\n",
        "\n",
        "    # Evaluate\n",
        "    val_loss, val_accuracy = model.evaluate(X_val_pad, y_val, verbose=1)\n",
        "    print(f\"✅ Fold {fold + 1} - Loss: {val_loss:.4f} | Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    y_val_probs = model.predict(X_val_pad)\n",
        "    y_val_pred = np.argmax(y_val_probs, axis=1)\n",
        "    y_val_true = np.argmax(y_val, axis=1)\n",
        "\n",
        "    f1 = f1_score(y_val_true, y_val_pred, average='weighted')\n",
        "    fold_accuracies.append(val_accuracy)\n",
        "    fold_f1_scores.append(f1)\n",
        "    print(f\"📊 Fold {fold + 1} - F1-Score: {f1:.4f}\")\n",
        "\n",
        "# ======================\n",
        "# ✅ 5. Final Results Summary\n",
        "# ======================\n",
        "print(\"\\n📈 Final GloVe GRU Model Results:\")\n",
        "print(f\"Mean Accuracy: {np.mean(fold_accuracies):.4f} ± {np.std(fold_accuracies):.4f}\")\n",
        "print(f\"Mean F1-Score: {np.mean(fold_f1_scores):.4f} ± {np.std(fold_f1_scores):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGB24DttFoSf",
        "outputId": "6e5d02fe-527e-4942-dcae-7dc5169572e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-04 10:25:17--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-04-04 10:25:17--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.00MB/s    in 2m 39s  \n",
            "\n",
            "2025-04-04 10:27:57 (5.16 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget 'https://nlp.stanford.edu/data/glove.6B.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPnm64xzH8M_",
        "outputId": "8d01604d-2f84-4489-bfbb-569fe6e6292e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ],
      "source": [
        "!unzip glove.6B.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BVls0KEBFfJ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WE_HUBhBF1O",
        "outputId": "c44da45b-395a-40bb-baa7-068fbe99afdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "# ONLY if you need them\n",
        "!pip install scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jakSqUrK-M7"
      },
      "source": [
        "This code is a hybrid approach that combines:\n",
        "\n",
        "✅ Stratified Train-Test Split (for final evaluation)\n",
        "\n",
        "✅ Stratified K-Fold Cross-Validation (for robust training)\n",
        "\n",
        "✅ Bidirectional LSTM model with early stopping\n",
        "\n",
        "✅ Basic label balance checks and predictions\n",
        "\n",
        "Let me annotate it with clear comments (inside the code) and summarize its differences from the previous ones at the end 👇"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVPz4HOHK-Wv",
        "outputId": "ba4ee430-9b1a-4d6e-d5f4-3a291ee49ea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📁 Training Fold 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 89ms/step - accuracy: 0.5034 - loss: 1.0086 - val_accuracy: 0.5371 - val_loss: 0.9638\n",
            "Epoch 2/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 89ms/step - accuracy: 0.5368 - loss: 0.9570 - val_accuracy: 0.5620 - val_loss: 0.9001\n",
            "Epoch 3/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 88ms/step - accuracy: 0.5830 - loss: 0.8788 - val_accuracy: 0.4711 - val_loss: 0.9991\n",
            "Epoch 4/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 89ms/step - accuracy: 0.5812 - loss: 0.8530 - val_accuracy: 0.6455 - val_loss: 0.7693\n",
            "Epoch 5/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 92ms/step - accuracy: 0.6816 - loss: 0.6919 - val_accuracy: 0.6599 - val_loss: 0.7577\n",
            "Epoch 6/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 92ms/step - accuracy: 0.7262 - loss: 0.6089 - val_accuracy: 0.6541 - val_loss: 0.7914\n",
            "Epoch 7/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 89ms/step - accuracy: 0.7651 - loss: 0.5583 - val_accuracy: 0.6532 - val_loss: 0.7714\n",
            "Epoch 8/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 89ms/step - accuracy: 0.8075 - loss: 0.4858 - val_accuracy: 0.6573 - val_loss: 0.8416\n",
            "Epoch 9/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 92ms/step - accuracy: 0.8305 - loss: 0.4366 - val_accuracy: 0.6536 - val_loss: 0.8778\n",
            "Epoch 10/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 89ms/step - accuracy: 0.8629 - loss: 0.3777 - val_accuracy: 0.6562 - val_loss: 1.0450\n",
            "\n",
            "📁 Training Fold 2/5\n",
            "Epoch 1/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 95ms/step - accuracy: 0.5046 - loss: 0.9920 - val_accuracy: 0.6328 - val_loss: 0.7925\n",
            "Epoch 2/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 90ms/step - accuracy: 0.6578 - loss: 0.7753 - val_accuracy: 0.6539 - val_loss: 0.8237\n",
            "Epoch 3/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 89ms/step - accuracy: 0.6691 - loss: 0.7638 - val_accuracy: 0.6818 - val_loss: 0.7462\n",
            "Epoch 4/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 89ms/step - accuracy: 0.7574 - loss: 0.5884 - val_accuracy: 0.6674 - val_loss: 0.7497\n",
            "Epoch 5/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 89ms/step - accuracy: 0.8015 - loss: 0.4980 - val_accuracy: 0.6809 - val_loss: 0.7783\n",
            "Epoch 6/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 89ms/step - accuracy: 0.8509 - loss: 0.3997 - val_accuracy: 0.6682 - val_loss: 0.8763\n",
            "Epoch 7/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 89ms/step - accuracy: 0.8885 - loss: 0.3159 - val_accuracy: 0.6634 - val_loss: 1.0580\n",
            "Epoch 8/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 89ms/step - accuracy: 0.9185 - loss: 0.2330 - val_accuracy: 0.6579 - val_loss: 1.2311\n",
            "\n",
            "📁 Training Fold 3/5\n",
            "Epoch 1/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 90ms/step - accuracy: 0.4898 - loss: 1.0040 - val_accuracy: 0.6368 - val_loss: 0.8289\n",
            "Epoch 2/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 89ms/step - accuracy: 0.6403 - loss: 0.8028 - val_accuracy: 0.5235 - val_loss: 0.9612\n",
            "Epoch 3/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 92ms/step - accuracy: 0.5605 - loss: 0.9250 - val_accuracy: 0.5936 - val_loss: 0.8370\n",
            "Epoch 4/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 90ms/step - accuracy: 0.6843 - loss: 0.7183 - val_accuracy: 0.6717 - val_loss: 0.7325\n",
            "Epoch 5/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 92ms/step - accuracy: 0.7408 - loss: 0.6183 - val_accuracy: 0.6850 - val_loss: 0.7147\n",
            "Epoch 6/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 92ms/step - accuracy: 0.7873 - loss: 0.5418 - val_accuracy: 0.6820 - val_loss: 0.7336\n",
            "Epoch 7/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 92ms/step - accuracy: 0.8259 - loss: 0.4495 - val_accuracy: 0.6665 - val_loss: 0.8165\n",
            "Epoch 8/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 89ms/step - accuracy: 0.8650 - loss: 0.3730 - val_accuracy: 0.6828 - val_loss: 0.8672\n",
            "Epoch 9/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 89ms/step - accuracy: 0.8974 - loss: 0.2965 - val_accuracy: 0.6780 - val_loss: 1.0304\n",
            "Epoch 10/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 89ms/step - accuracy: 0.9222 - loss: 0.2355 - val_accuracy: 0.6685 - val_loss: 1.1087\n",
            "\n",
            "📁 Training Fold 4/5\n",
            "Epoch 1/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 90ms/step - accuracy: 0.5100 - loss: 0.9786 - val_accuracy: 0.6024 - val_loss: 0.8405\n",
            "Epoch 2/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 94ms/step - accuracy: 0.6517 - loss: 0.7741 - val_accuracy: 0.6549 - val_loss: 0.7685\n",
            "Epoch 3/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 89ms/step - accuracy: 0.7278 - loss: 0.6459 - val_accuracy: 0.6826 - val_loss: 0.7348\n",
            "Epoch 4/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 90ms/step - accuracy: 0.7863 - loss: 0.5307 - val_accuracy: 0.6916 - val_loss: 0.7475\n",
            "Epoch 5/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 89ms/step - accuracy: 0.8339 - loss: 0.4348 - val_accuracy: 0.6821 - val_loss: 0.8378\n",
            "Epoch 6/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 89ms/step - accuracy: 0.8751 - loss: 0.3420 - val_accuracy: 0.6664 - val_loss: 0.9414\n",
            "Epoch 7/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 89ms/step - accuracy: 0.9114 - loss: 0.2581 - val_accuracy: 0.6868 - val_loss: 1.1787\n",
            "Epoch 8/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 89ms/step - accuracy: 0.9353 - loss: 0.1909 - val_accuracy: 0.6655 - val_loss: 1.2724\n",
            "\n",
            "📁 Training Fold 5/5\n",
            "Epoch 1/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 91ms/step - accuracy: 0.5112 - loss: 0.9782 - val_accuracy: 0.6092 - val_loss: 0.8150\n",
            "Epoch 2/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 90ms/step - accuracy: 0.6396 - loss: 0.8139 - val_accuracy: 0.6362 - val_loss: 0.7786\n",
            "Epoch 3/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 90ms/step - accuracy: 0.7238 - loss: 0.6547 - val_accuracy: 0.6672 - val_loss: 0.7570\n",
            "Epoch 4/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 93ms/step - accuracy: 0.7904 - loss: 0.5261 - val_accuracy: 0.6594 - val_loss: 0.8287\n",
            "Epoch 5/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 90ms/step - accuracy: 0.8389 - loss: 0.4170 - val_accuracy: 0.6559 - val_loss: 0.8562\n",
            "Epoch 6/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 92ms/step - accuracy: 0.8758 - loss: 0.3342 - val_accuracy: 0.6532 - val_loss: 1.0657\n",
            "Epoch 7/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 92ms/step - accuracy: 0.9062 - loss: 0.2633 - val_accuracy: 0.6444 - val_loss: 1.2348\n",
            "Epoch 8/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 89ms/step - accuracy: 0.9327 - loss: 0.2016 - val_accuracy: 0.6493 - val_loss: 1.2744\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.6685 - loss: 0.7552\n",
            "\n",
            "🧪 Test Loss: 0.7593, Test Accuracy: 0.6674\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366ms/step\n",
            "Predictions (probabilities): [[0.00434475 0.01805323 0.97760195]\n",
            " [0.7929268  0.13350934 0.07356386]\n",
            " [0.3238767  0.58569306 0.09043022]]\n",
            "Predicted Sentiment Labels: [ 1 -1  0]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# ======================\n",
        "# ✅ Load & clean the dataset ;))\n",
        "# ======================\n",
        "def load_clean_csv(file_path):\n",
        "    cleaned_rows = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        header = next(reader)\n",
        "        for row in reader:\n",
        "            if len(row) == len(header):\n",
        "                cleaned_rows.append(row)\n",
        "    return pd.DataFrame(cleaned_rows, columns=header)\n",
        "\n",
        "data = load_clean_csv(\"/content/semantic_tech_real50.csv\")\n",
        "data = data[['review_text', 'sentiment']]\n",
        "data.columns = ['text', 'label']\n",
        "data = data.dropna()\n",
        "data['text'] = data['text'].astype(str).str.strip()\n",
        "data['label'] = data['label'].astype(int) + 1  # (-1,0,1) → (0,1,2)\n",
        "\n",
        "# ======================\n",
        "# ✅ Set parameters ;))\n",
        "# ======================\n",
        "VOCAB_SIZE = 30000\n",
        "EMBEDDING_DIM = 300\n",
        "MAX_LENGTH = 200\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "\n",
        "# ======================\n",
        "# ✅ Prepare labels and input\n",
        "# ======================\n",
        "comments = data['text'].values\n",
        "labels = to_categorical(data['label'].values, num_classes=3)\n",
        "\n",
        "# ======================\n",
        "# ✅ Stratified Train-Test Split ;))\n",
        "# ======================\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    comments,\n",
        "    labels,\n",
        "    test_size=0.2,\n",
        "    stratify=np.argmax(labels, axis=1),\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ======================\n",
        "# ✅ Tokenizer & Padding ;))\n",
        "# ======================\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=MAX_LENGTH, padding='post')\n",
        "X_test_padded = pad_sequences(X_test_seq, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "# ======================\n",
        "# ✅ Early Stopping ;))\n",
        "# ======================\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# ======================\n",
        "# ✅ Stratified K-Fold Training (on train set only)\n",
        "# ======================\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_no = 1\n",
        "\n",
        "for train_idx, val_idx in kfold.split(X_train_padded, np.argmax(y_train, axis=1)):\n",
        "    print(f\"\\n📁 Training Fold {fold_no}/5\")\n",
        "\n",
        "    # ✅ New model for each fold (important!)\n",
        "    model = Sequential([\n",
        "        Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH),\n",
        "        Bidirectional(LSTM(256, activation='tanh', return_sequences=True)),\n",
        "        Dropout(0.5),\n",
        "        Bidirectional(LSTM(128, activation='tanh')),\n",
        "        Dropout(0.5),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(3, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # ✅ Train on current fold\n",
        "    history = model.fit(\n",
        "        X_train_padded[train_idx], y_train[train_idx],\n",
        "        validation_data=(X_train_padded[val_idx], y_train[val_idx]),\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    fold_no += 1\n",
        "\n",
        "# ======================\n",
        "# ✅ Final Evaluation on Test Set ;))\n",
        "# ======================\n",
        "test_loss, test_accuracy = model.evaluate(X_test_padded, y_test, verbose=1)\n",
        "print(f\"\\n🧪 Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# ======================\n",
        "# ✅ Save the final model\n",
        "# ======================\n",
        "model.save(\"lstm_yelp_sentiment_model_crossval.keras\")\n",
        "\n",
        "# ======================\n",
        "# ✅ Predict on sample inputs ;))\n",
        "# ======================\n",
        "sample_comments = [\n",
        "    \"I love this phone. Battery life is amazing!\",\n",
        "    \"Worst customer service I've ever had.\",\n",
        "    \"It’s just okay. Nothing special.\"\n",
        "]\n",
        "\n",
        "sample_seq = tokenizer.texts_to_sequences(sample_comments)\n",
        "sample_pad = pad_sequences(sample_seq, maxlen=MAX_LENGTH, padding='post')\n",
        "predictions = model.predict(sample_pad)\n",
        "\n",
        "print(\"Predictions (probabilities):\", predictions)\n",
        "predicted_labels = np.argmax(predictions, axis=1) - 1  # shift back → (-1, 0, 1)\n",
        "print(\"Predicted Sentiment Labels:\", predicted_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GHLRCCtPsuL"
      },
      "source": [
        "we are herer , do it tommorroe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXVFkYLfPsu4"
      },
      "source": [
        "✅ Final Version: Attention LSTM with Yelp Sentiment Dataset\n",
        "python\n",
        "Copy\n",
        "Edit\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUFCyYDaWiK_"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsXjl3-6Ps9F",
        "outputId": "a9cfc4f5-1db5-41c3-9657-e53f67aa0824"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class Weights: {0: np.float64(1.127321175784828), 1: np.float64(1.298235519754507), 2: np.float64(0.744787368652693)}\n",
            "\n",
            "📁 Training Fold 1/5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 66ms/step - accuracy: 0.4975 - loss: 0.9836 - val_accuracy: 0.6877 - val_loss: 0.7135\n",
            "Epoch 2/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 66ms/step - accuracy: 0.7353 - loss: 0.6666 - val_accuracy: 0.7049 - val_loss: 0.6885\n",
            "Epoch 3/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 65ms/step - accuracy: 0.8066 - loss: 0.5131 - val_accuracy: 0.6754 - val_loss: 0.7654\n",
            "Epoch 4/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 66ms/step - accuracy: 0.8719 - loss: 0.3714 - val_accuracy: 0.6735 - val_loss: 0.9218\n",
            "Epoch 5/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 71ms/step - accuracy: 0.9158 - loss: 0.2423 - val_accuracy: 0.6664 - val_loss: 1.1164\n",
            "Epoch 6/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 71ms/step - accuracy: 0.9453 - loss: 0.1674 - val_accuracy: 0.6594 - val_loss: 1.5112\n",
            "Epoch 7/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 71ms/step - accuracy: 0.9647 - loss: 0.1118 - val_accuracy: 0.6570 - val_loss: 1.6090\n",
            "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.7077 - loss: 0.6767\n",
            "✅ Fold 1: Val Loss = 0.6885, Accuracy = 0.7049\n",
            "\n",
            "📁 Training Fold 2/5\n",
            "Epoch 1/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 67ms/step - accuracy: 0.5004 - loss: 0.9859 - val_accuracy: 0.6756 - val_loss: 0.7467\n",
            "Epoch 2/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 70ms/step - accuracy: 0.7397 - loss: 0.6653 - val_accuracy: 0.6876 - val_loss: 0.7140\n",
            "Epoch 3/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 70ms/step - accuracy: 0.8180 - loss: 0.4992 - val_accuracy: 0.6906 - val_loss: 0.7473\n",
            "Epoch 4/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 70ms/step - accuracy: 0.8790 - loss: 0.3571 - val_accuracy: 0.6749 - val_loss: 0.9708\n",
            "Epoch 5/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 72ms/step - accuracy: 0.9224 - loss: 0.2355 - val_accuracy: 0.6718 - val_loss: 1.1290\n",
            "Epoch 6/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 70ms/step - accuracy: 0.9510 - loss: 0.1545 - val_accuracy: 0.6682 - val_loss: 1.4229\n",
            "Epoch 7/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 70ms/step - accuracy: 0.9663 - loss: 0.1099 - val_accuracy: 0.6688 - val_loss: 1.4597\n",
            "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - accuracy: 0.6832 - loss: 0.7292\n",
            "✅ Fold 2: Val Loss = 0.7140, Accuracy = 0.6876\n",
            "\n",
            "📁 Training Fold 3/5\n",
            "Epoch 1/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 68ms/step - accuracy: 0.5003 - loss: 1.0050 - val_accuracy: 0.6754 - val_loss: 0.7317\n",
            "Epoch 2/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 70ms/step - accuracy: 0.7256 - loss: 0.6828 - val_accuracy: 0.6768 - val_loss: 0.7089\n",
            "Epoch 3/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 71ms/step - accuracy: 0.8044 - loss: 0.5230 - val_accuracy: 0.6900 - val_loss: 0.7496\n",
            "Epoch 4/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 66ms/step - accuracy: 0.8702 - loss: 0.3695 - val_accuracy: 0.6734 - val_loss: 0.8854\n",
            "Epoch 5/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 71ms/step - accuracy: 0.9191 - loss: 0.2433 - val_accuracy: 0.6692 - val_loss: 1.1296\n",
            "Epoch 6/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 71ms/step - accuracy: 0.9481 - loss: 0.1572 - val_accuracy: 0.6541 - val_loss: 1.4030\n",
            "Epoch 7/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 66ms/step - accuracy: 0.9635 - loss: 0.1143 - val_accuracy: 0.6506 - val_loss: 1.4694\n",
            "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - accuracy: 0.6750 - loss: 0.7109\n",
            "✅ Fold 3: Val Loss = 0.7089, Accuracy = 0.6768\n",
            "\n",
            "📁 Training Fold 4/5\n",
            "Epoch 1/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 72ms/step - accuracy: 0.4786 - loss: 1.0091 - val_accuracy: 0.6898 - val_loss: 0.7087\n",
            "Epoch 2/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 71ms/step - accuracy: 0.7248 - loss: 0.6837 - val_accuracy: 0.6911 - val_loss: 0.7060\n",
            "Epoch 3/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 70ms/step - accuracy: 0.8012 - loss: 0.5287 - val_accuracy: 0.7053 - val_loss: 0.7494\n",
            "Epoch 4/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 66ms/step - accuracy: 0.8727 - loss: 0.3651 - val_accuracy: 0.6915 - val_loss: 0.8681\n",
            "Epoch 5/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 66ms/step - accuracy: 0.9206 - loss: 0.2403 - val_accuracy: 0.6852 - val_loss: 1.1192\n",
            "Epoch 6/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 66ms/step - accuracy: 0.9483 - loss: 0.1617 - val_accuracy: 0.6826 - val_loss: 1.3997\n",
            "Epoch 7/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 71ms/step - accuracy: 0.9635 - loss: 0.1135 - val_accuracy: 0.6670 - val_loss: 1.4422\n",
            "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.6948 - loss: 0.7040\n",
            "✅ Fold 4: Val Loss = 0.7060, Accuracy = 0.6911\n",
            "\n",
            "📁 Training Fold 5/5\n",
            "Epoch 1/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 72ms/step - accuracy: 0.4986 - loss: 0.9930 - val_accuracy: 0.6892 - val_loss: 0.7067\n",
            "Epoch 2/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 66ms/step - accuracy: 0.7307 - loss: 0.6677 - val_accuracy: 0.7067 - val_loss: 0.6830\n",
            "Epoch 3/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 66ms/step - accuracy: 0.8067 - loss: 0.5192 - val_accuracy: 0.6910 - val_loss: 0.7518\n",
            "Epoch 4/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 66ms/step - accuracy: 0.8745 - loss: 0.3593 - val_accuracy: 0.6810 - val_loss: 0.8428\n",
            "Epoch 5/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 71ms/step - accuracy: 0.9217 - loss: 0.2316 - val_accuracy: 0.6585 - val_loss: 1.2095\n",
            "Epoch 6/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 71ms/step - accuracy: 0.9479 - loss: 0.1599 - val_accuracy: 0.6697 - val_loss: 1.3514\n",
            "Epoch 7/30\n",
            "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 67ms/step - accuracy: 0.9688 - loss: 0.1010 - val_accuracy: 0.6599 - val_loss: 1.4865\n",
            "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - accuracy: 0.7077 - loss: 0.6906\n",
            "✅ Fold 5: Val Loss = 0.6830, Accuracy = 0.7067\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.7033 - loss: 0.6890\n",
            "\n",
            "🧪 Test Loss: 0.6983, Test Accuracy: 0.7039\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step\n",
            "📊 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.68      0.73      0.71      3002\n",
            "     Neutral       0.52      0.51      0.52      2607\n",
            "    Positive       0.83      0.80      0.81      4545\n",
            "\n",
            "    accuracy                           0.70     10154\n",
            "   macro avg       0.68      0.68      0.68     10154\n",
            "weighted avg       0.71      0.70      0.70     10154\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Bidirectional, Attention, Concatenate\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# ======================\n",
        "# ✅ 1. Load and clean your dataset\n",
        "# ======================\n",
        "def load_clean_csv(file_path):\n",
        "    cleaned_rows = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        header = next(reader)\n",
        "        for row in reader:\n",
        "            if len(row) == len(header):\n",
        "                cleaned_rows.append(row)\n",
        "    return pd.DataFrame(cleaned_rows, columns=header)\n",
        "\n",
        "df = load_clean_csv(\"/content/semantic_tech_real50.csv\")\n",
        "df = df[['review_text', 'sentiment']]\n",
        "df.columns = ['text', 'label']\n",
        "df = df.dropna()\n",
        "df['text'] = df['text'].astype(str).str.strip()\n",
        "df['label'] = df['label'].astype(int)\n",
        "\n",
        "# ======================\n",
        "# ✅ 2. Train/val/test split before CV\n",
        "# ======================\n",
        "train_val_df, test_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    stratify=df['label'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ======================\n",
        "# ✅ 3. Prepare tokenizer and sequences\n",
        "# ======================\n",
        "VOCAB_SIZE = 30000\n",
        "EMBEDDING_DIM = 300\n",
        "MAX_LENGTH = 200\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_val_df['text'])\n",
        "\n",
        "def prepare_sequences(texts):\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    return pad_sequences(sequences, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "X = prepare_sequences(train_val_df['text'])\n",
        "y = train_val_df['label'].astype(int).values + 1  # shift (-1,0,1) → (0,1,2)\n",
        "y_cat = to_categorical(y, num_classes=3)\n",
        "\n",
        "# ======================\n",
        "# ✅ 4. Compute class weights\n",
        "# ======================\n",
        "class_weights_array = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_val_df['label']),\n",
        "    y=train_val_df['label']\n",
        ")\n",
        "class_weights_dict = {i: weight for i, weight in enumerate(class_weights_array)}\n",
        "print(\"Class Weights:\", class_weights_dict)\n",
        "\n",
        "# ======================\n",
        "# ✅ 5. Define the model (with Attention)\n",
        "# ======================\n",
        "def create_attention_lstm_model():\n",
        "    inputs = Input(shape=(MAX_LENGTH,))\n",
        "    x = Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH)(inputs)\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
        "    attention = Attention()([x, x])\n",
        "    x = Concatenate()([x, attention])\n",
        "    x = Bidirectional(LSTM(64))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(3, activation='softmax')(x)\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# ======================\n",
        "# ✅ 6. Stratified K-Fold Cross-Validation\n",
        "# ======================\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_no = 1\n",
        "results = []\n",
        "\n",
        "for train_idx, val_idx in kfold.split(X, y):\n",
        "    print(f\"\\n📁 Training Fold {fold_no}/5\")\n",
        "\n",
        "    X_train, X_val = X[train_idx], X[val_idx]\n",
        "    y_train, y_val = y_cat[train_idx], y_cat[val_idx]\n",
        "\n",
        "    model = create_attention_lstm_model()\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_weight=class_weights_dict,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=1)\n",
        "    print(f\"✅ Fold {fold_no}: Val Loss = {val_loss:.4f}, Accuracy = {val_accuracy:.4f}\")\n",
        "    results.append((val_loss, val_accuracy))\n",
        "    fold_no += 1\n",
        "\n",
        "# ======================\n",
        "# ✅ 7. Final Evaluation on Held-Out Test Set\n",
        "# ======================\n",
        "X_test = prepare_sequences(test_df['text'])\n",
        "y_test = to_categorical(test_df['label'].astype(int).values + 1, num_classes=3)\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(f\"\\n🧪 Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# ======================\n",
        "# ✅ 8. Classification Report\n",
        "# ======================\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1) - 1  # shift back to original labels\n",
        "y_true = test_df['label'].astype(int).values\n",
        "\n",
        "print(\"📊 Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n",
        "\n",
        "# ======================\n",
        "# ✅ 9. Save the final model\n",
        "# ======================\n",
        "model.save(\"attention_lstm_yelp_sentiment.keras\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVGT8FyAWjed"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIOOanJiWjhH"
      },
      "source": [
        "This code is very similar to the previous Attention-LSTM setup, but now uses pre-trained GloVe embeddings (like you asked earlier). Let's walk through what makes it different, and then I'll give you the fully compatible version for your Apple/Samsung Yelp dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMjWvR-5WjwF",
        "outputId": "2f29a7cc-8da1-4d87-8ac8-c5696226208f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class Weights: {0: np.float64(0.6783881596715822), 1: np.float64(1.6397702049875964), 2: np.float64(1.0916123424598)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 102ms/step - accuracy: 0.4184 - loss: 1.0870 - val_accuracy: 0.6458 - val_loss: 0.8419\n",
            "Epoch 2/15\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 100ms/step - accuracy: 0.6447 - loss: 0.8478 - val_accuracy: 0.6378 - val_loss: 0.8631\n",
            "Epoch 3/15\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 101ms/step - accuracy: 0.6884 - loss: 0.7710 - val_accuracy: 0.6935 - val_loss: 0.7096\n",
            "Epoch 4/15\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 103ms/step - accuracy: 0.7238 - loss: 0.7003 - val_accuracy: 0.6716 - val_loss: 0.7262\n",
            "Epoch 5/15\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 101ms/step - accuracy: 0.7476 - loss: 0.6604 - val_accuracy: 0.6935 - val_loss: 0.7327\n",
            "Epoch 6/15\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 103ms/step - accuracy: 0.7609 - loss: 0.6038 - val_accuracy: 0.7085 - val_loss: 0.6848\n",
            "Epoch 7/15\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 101ms/step - accuracy: 0.7742 - loss: 0.5590 - val_accuracy: 0.6517 - val_loss: 0.8008\n",
            "Epoch 8/15\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 100ms/step - accuracy: 0.8199 - loss: 0.4777 - val_accuracy: 0.6776 - val_loss: 0.7699\n",
            "Epoch 9/15\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 102ms/step - accuracy: 0.8412 - loss: 0.4234 - val_accuracy: 0.5642 - val_loss: 0.9752\n",
            "Epoch 10/15\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 102ms/step - accuracy: 0.7582 - loss: 0.5963 - val_accuracy: 0.6488 - val_loss: 0.9209\n",
            "Epoch 11/15\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 102ms/step - accuracy: 0.8737 - loss: 0.3408 - val_accuracy: 0.6856 - val_loss: 1.0541\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.7226 - loss: 0.6121\n",
            "\n",
            "🧪 Test Loss: 0.6441, Test Accuracy: 0.7050\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step\n",
            "📊 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.83      0.78      0.80      1234\n",
            "     Neutral       0.39      0.44      0.41       511\n",
            "    Positive       0.75      0.76      0.76       767\n",
            "\n",
            "    accuracy                           0.71      2512\n",
            "   macro avg       0.66      0.66      0.66      2512\n",
            "weighted avg       0.72      0.71      0.71      2512\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Bidirectional, Attention, Concatenate\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# ======================\n",
        "# ✅ Load and clean your dataset\n",
        "# ======================\n",
        "def load_clean_csv(file_path):\n",
        "    cleaned_rows = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        header = next(reader)\n",
        "        for row in reader:\n",
        "            if len(row) == len(header):\n",
        "                cleaned_rows.append(row)\n",
        "    return pd.DataFrame(cleaned_rows, columns=header)\n",
        "\n",
        "df = load_clean_csv(\"/content/semantic_tech_real50.csv\")\n",
        "df = df[['review_text', 'sentiment']]\n",
        "df.columns = ['text', 'label']\n",
        "df = df.dropna()\n",
        "df['text'] = df['text'].astype(str).str.strip()\n",
        "df['label'] = df['label'].astype(int)\n",
        "\n",
        "# ======================\n",
        "# ✅ Set model parameters\n",
        "# ======================\n",
        "VOCAB_SIZE = 30000\n",
        "EMBEDDING_DIM = 300\n",
        "MAX_LENGTH = 200\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 15\n",
        "GLOVE_PATH = \"glove.6B.300d.txt\"  # Make sure this file is uploaded\n",
        "\n",
        "# ======================\n",
        "# ✅ Tokenizer & sequences\n",
        "# ======================\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "\n",
        "def prepare_sequences(texts):\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    return pad_sequences(sequences, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "X = prepare_sequences(df['text'])\n",
        "y = df['label'].values + 1  # Shift -1/0/1 → 0/1/2\n",
        "y_cat = to_categorical(y, num_classes=3)\n",
        "\n",
        "# ======================\n",
        "# ✅ Load GloVe embeddings manually\n",
        "# ======================\n",
        "def load_glove_embeddings(glove_path, embedding_dim, tokenizer, vocab_size):\n",
        "    embeddings_index = {}\n",
        "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefficients = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefficients\n",
        "\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if i < vocab_size:\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix\n",
        "\n",
        "embedding_matrix = load_glove_embeddings(GLOVE_PATH, EMBEDDING_DIM, tokenizer, VOCAB_SIZE)\n",
        "\n",
        "# ======================\n",
        "# ✅ Compute class weights\n",
        "# ======================\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(df['label']),\n",
        "    y=df['label']\n",
        ")\n",
        "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "print(\"Class Weights:\", class_weights_dict)\n",
        "\n",
        "# ======================\n",
        "# ✅ Define the Attention-LSTM model with GloVe\n",
        "# ======================\n",
        "def create_robust_model():\n",
        "    inputs = Input(shape=(MAX_LENGTH,))\n",
        "    embedding_layer = Embedding(\n",
        "        VOCAB_SIZE,\n",
        "        EMBEDDING_DIM,\n",
        "        embeddings_initializer=Constant(embedding_matrix),\n",
        "        input_length=MAX_LENGTH,\n",
        "        trainable=False  # ❄️ Keep GloVe frozen\n",
        "    )(inputs)\n",
        "\n",
        "    x = Bidirectional(LSTM(256, return_sequences=True))(embedding_layer)\n",
        "    attention = Attention()([x, x])\n",
        "    x = Concatenate()([x, attention])\n",
        "    x = Bidirectional(LSTM(128))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(3, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# ======================\n",
        "# ✅ Train/Test Split + Model Training\n",
        "# ======================\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_cat, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "model = create_robust_model()\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.1,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ======================\n",
        "# ✅ Final Test Evaluation\n",
        "# ======================\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(f\"\\n🧪 Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# ======================\n",
        "# ✅ Classification Report\n",
        "# ======================\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1) - 1  # Back to -1/0/1\n",
        "y_true = np.argmax(y_test, axis=1) - 1\n",
        "\n",
        "print(\"📊 Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n",
        "\n",
        "# ======================\n",
        "# ✅ Save the Model\n",
        "# ======================\n",
        "model.save(\"attention_lstm_glove_yelp.keras\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIQtr2BoYsn7"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcxk4MDYYsvX"
      },
      "source": [
        "Solution Strategy:\n",
        "We'll combine the most effective improvements into a single version:\n",
        "\n",
        "✅ Use GloVe embeddings (pretrained)\n",
        "✅ Use Bidirectional LSTM + Attention\n",
        "✅ Apply Focal Loss (to focus on hard-to-classify Neutral examples)\n",
        "✅ Use Stratified Train-Test Split\n",
        "✅ Include Confusion Matrix + Report to track Neutral behavior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g_xe9iE2Ys5_",
        "outputId": "1c52e422-2cfc-45c4-c1f6-5d6e2ba6c9ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m572/572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 112ms/step - accuracy: 0.4787 - loss: 0.1122 - val_accuracy: 0.6356 - val_loss: 0.0832\n",
            "Epoch 2/20\n",
            "\u001b[1m572/572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 104ms/step - accuracy: 0.6356 - loss: 0.0861 - val_accuracy: 0.6787 - val_loss: 0.0734\n",
            "Epoch 3/20\n",
            "\u001b[1m572/572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 103ms/step - accuracy: 0.6810 - loss: 0.0740 - val_accuracy: 0.6849 - val_loss: 0.0708\n",
            "Epoch 4/20\n",
            "\u001b[1m572/572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 103ms/step - accuracy: 0.7019 - loss: 0.0680 - val_accuracy: 0.6940 - val_loss: 0.0700\n",
            "Epoch 5/20\n",
            "\u001b[1m572/572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 103ms/step - accuracy: 0.7270 - loss: 0.0595 - val_accuracy: 0.7083 - val_loss: 0.0679\n",
            "Epoch 6/20\n",
            "\u001b[1m572/572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 103ms/step - accuracy: 0.7459 - loss: 0.0533 - val_accuracy: 0.6800 - val_loss: 0.0779\n",
            "Epoch 7/20\n",
            "\u001b[1m572/572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 102ms/step - accuracy: 0.7572 - loss: 0.0484 - val_accuracy: 0.7075 - val_loss: 0.0769\n",
            "Epoch 8/20\n",
            "\u001b[1m572/572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 103ms/step - accuracy: 0.7841 - loss: 0.0390 - val_accuracy: 0.6861 - val_loss: 0.0927\n",
            "Epoch 9/20\n",
            "\u001b[1m572/572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 102ms/step - accuracy: 0.8072 - loss: 0.0338 - val_accuracy: 0.6851 - val_loss: 0.1197\n",
            "Epoch 10/20\n",
            "\u001b[1m572/572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 102ms/step - accuracy: 0.8143 - loss: 0.0305 - val_accuracy: 0.7043 - val_loss: 0.1136\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step - accuracy: 0.7021 - loss: 0.0699\n",
            "\n",
            "🧪 Test Loss: 0.0700, Test Accuracy: 0.7019\n",
            "\u001b[1m318/318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step\n",
            "📊 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.67      0.76      0.72      3002\n",
            "     Neutral       0.53      0.42      0.47      2607\n",
            "    Positive       0.80      0.83      0.81      4545\n",
            "\n",
            "    accuracy                           0.70     10154\n",
            "   macro avg       0.67      0.67      0.66     10154\n",
            "weighted avg       0.69      0.70      0.69     10154\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Bidirectional, Attention, Concatenate\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow.keras.backend as K\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ======================\n",
        "# ✅ 1. Load and clean your dataset\n",
        "# ======================\n",
        "def load_clean_csv(file_path):\n",
        "    cleaned_rows = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        header = next(reader)\n",
        "        for row in reader:\n",
        "            if len(row) == len(header):\n",
        "                cleaned_rows.append(row)\n",
        "    return pd.DataFrame(cleaned_rows, columns=header)\n",
        "\n",
        "df = load_clean_csv(\"/content/semantic_tech_real50.csv\")\n",
        "df = df[['review_text', 'sentiment']]\n",
        "df.columns = ['text', 'label']\n",
        "df = df.dropna()\n",
        "df['text'] = df['text'].astype(str).str.strip()\n",
        "df['label'] = df['label'].astype(int)\n",
        "\n",
        "# ======================\n",
        "# ✅ 2. Parameters\n",
        "# ======================\n",
        "VOCAB_SIZE = 30000\n",
        "EMBEDDING_DIM = 300\n",
        "MAX_LENGTH = 200\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "GLOVE_PATH = \"glove.6B.300d.txt\"\n",
        "\n",
        "# ======================\n",
        "# ✅ 3. Tokenizer & sequence prep\n",
        "# ======================\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(df['text'])\n",
        "\n",
        "def prepare_sequences(texts):\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    return pad_sequences(sequences, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "X = prepare_sequences(df['text'])\n",
        "y = df['label'].values + 1  # Shift -1,0,1 → 0,1,2\n",
        "y_cat = to_categorical(y, num_classes=3)\n",
        "\n",
        "# ======================\n",
        "# ✅ 4. Load GloVe embeddings\n",
        "# ======================\n",
        "def load_glove_embeddings(glove_path, embedding_dim, tokenizer, vocab_size):\n",
        "    embeddings_index = {}\n",
        "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefficients = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefficients\n",
        "\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if i < vocab_size:\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix\n",
        "\n",
        "embedding_matrix = load_glove_embeddings(GLOVE_PATH, EMBEDDING_DIM, tokenizer, VOCAB_SIZE)\n",
        "\n",
        "# ======================\n",
        "# ✅ 5. Focal Loss\n",
        "# ======================\n",
        "from tensorflow.keras.losses import Loss\n",
        "\n",
        "class FocalLoss(Loss):\n",
        "    def __init__(self, gamma=2., alpha=0.25):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        epsilon = K.epsilon()\n",
        "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
        "        cross_entropy = -y_true * K.log(y_pred)\n",
        "        weights = self.alpha * K.pow(1 - y_pred, self.gamma)\n",
        "        loss = weights * cross_entropy\n",
        "        return K.sum(loss, axis=1)\n",
        "\n",
        "# ======================\n",
        "# ✅ 6. Define the robust model\n",
        "# ======================\n",
        "def create_robust_model():\n",
        "    inputs = Input(shape=(MAX_LENGTH,))\n",
        "    embedding_layer = Embedding(\n",
        "        VOCAB_SIZE,\n",
        "        EMBEDDING_DIM,\n",
        "        embeddings_initializer=Constant(embedding_matrix),\n",
        "        input_length=MAX_LENGTH,\n",
        "        trainable=False\n",
        "    )(inputs)\n",
        "\n",
        "    x = Bidirectional(LSTM(256, return_sequences=True))(embedding_layer)\n",
        "    attention = Attention()([x, x])\n",
        "    x = Concatenate()([x, attention])\n",
        "    x = Bidirectional(LSTM(128))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(3, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(optimizer='adam', loss=FocalLoss(), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# ======================\n",
        "# ✅ 7. Stratified Train-Test split\n",
        "# ======================\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_cat, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "model = create_robust_model()\n",
        "\n",
        "# ======================\n",
        "# ✅ 8. Train the model\n",
        "# ======================\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.1,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ======================\n",
        "# ✅ 9. Evaluate & report\n",
        "# ======================\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(f\"\\n🧪 Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1) - 1\n",
        "y_true = np.argmax(y_test, axis=1) - 1\n",
        "\n",
        "print(\"📊 Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n",
        "\n",
        "# ======================\n",
        "# ✅ 10. Confusion Matrix\n",
        "# ======================\n",
        "cm = confusion_matrix(y_true, y_pred, labels=[-1, 0, 1])\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Neg\", \"Neu\", \"Pos\"], yticklabels=[\"Neg\", \"Neu\", \"Pos\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# ======================\n",
        "# ✅ 11. Save model\n",
        "# ======================\n",
        "model.save(\"attention_lstm_focal_yelp.keras\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}